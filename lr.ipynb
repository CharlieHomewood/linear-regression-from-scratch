{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db98977f",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb98d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf298f1",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc317fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Student_Performance.csv\")\n",
    "\n",
    "# recoding categorical variable\n",
    "df[\"Extracurricular Activities\"] = df[\"Extracurricular Activities\"].replace({\"Yes\": 1, \"No\": 0}).astype(int)\n",
    "\n",
    "# randomly shuffle rows in df, for fair train-test split\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "feature_matrix = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf4595",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Aim: We want to create a  multiple linear regression model predict the `performance index` values for observations from unseen samples. \n",
    "\n",
    " - This involves optimally fitting a straight-line to our features in some $d$-dimensional space \n",
    "    - 5 dimensions in our case, since we have 5 features in our data set and 1 target variable.\n",
    "\n",
    "This multiple linear regression model takes the form:\n",
    "\n",
    "$$ \\hat{y}=Xw + b = w_{1}x_{1} + \\dots + w_{d}x_{d} + b $$\n",
    "\n",
    "Our estimates for `performance index` are $\\hat{y} \\in \\mathbb{R}^{n}$ \n",
    "\n",
    " - The notation $\\hat{y} \\in \\mathbb{R}^{d}$ means that $\\hat{y}$ is a $d$-dimensional vector, where each element of the vector is some real number \n",
    "     - $\\mathbb{R}$ is just mathematical notation for \"the set of all real numbers\", and $\\in$ means \"$\\hat{y}$ is a member of the set $\\mathbb{R}^{d}$ \".\n",
    "\n",
    "$$ \\hat{y} = \\begin{bmatrix} \\hat{y_{1}} = x_{1}^{\\top}w+b \\\\ \\hat{y_{2}} = x_{2}^{\\top}w+b \\\\ \\vdots \\\\ \\hat{y_{d}} = x_{d}^{\\top}w+b \\end{bmatrix} $$\n",
    "\n",
    " - As such, each element of the $\\hat{y}$ vector corresponds to the estimate for each `performance index` value of each observation in our data, using a straight-line equation.\n",
    " - $x_{d}^{\\top}$ refers to the transpose of each column from our design matrix. As such, these are row vectors which correspond to the set of values for each feature for a given observation in our data set.\n",
    "\n",
    "Our design matrix is $X \\in \\mathbb{R}^{n \\times d}$\n",
    "\n",
    "$$ X = \\begin{bmatrix} x_{11} & x_{12} & \\dots & x_{1d} \\\\ x_{21} & x_{22} & \\dots & x_{1d} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\dots & x_{nd} \\\\ \\end{bmatrix} $$\n",
    "\n",
    "Our optimal weights are $w \\in \\mathbb{R}^{d}$ \n",
    "\n",
    "$$ w = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{d} \\end{bmatrix} $$\n",
    "\n",
    "Our bias constant is given by $b \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f7a7f",
   "metadata": {},
   "source": [
    "## Simplfying the Formula: Vectorisation\n",
    "\n",
    "Currently, the regression model takes the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X^{\\top}w+b\n",
    "$$\n",
    "\n",
    "This is somewhat cumbersome to calculate as it will require optimising $w$ and $b$ separately. It would instead be better to optimise them simultaneously. We can achieve this via vectorisation. We can combine the bias constant into our weight vector and augment our design matrix by appending a column of $1\\text{s}$. \n",
    "\n",
    "The bias constant is our $y$-intercept for our straight line which we are trying to fit to our data. \n",
    "\n",
    "Geometrically, this is equivalent to translating the straight line up or down according to the value of $b$. Generalised to more than 2 dimensions, this straight line is actually a hyperplane in $d$-dimensions, but the same logic in 2-dimensions still applies.\n",
    "\n",
    "We can therefore think of this bias term as another weight in our model, but not one which rotates the hyperplane, but instead just translates it. Since this bias only translates the hyperplane, not rotates, it must move every point on the hyperplane uniformly in the same direction (whereas a rotation of the hyperplane would move different points by varying amounts - imagine rotating a line and looking at how far points near the axis of rotation move compared to those further away). \n",
    "\n",
    "Thus, to combine this bias term into our weights vector, we need to provide a constant dimension in our design matrix for the bias term to act along, hence the column of $1\\text{s}$.\n",
    "\n",
    "Doing this makes calculating $\\hat{y}$ more straightforward as it is now a simple matrix-vector multiplication. \n",
    "\n",
    "$$ \\hat{y} = \\tilde{X}\\tilde{w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3950d2ee",
   "metadata": {},
   "source": [
    "# Finding the Optimal Weights (+ Bias): Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da165c",
   "metadata": {},
   "source": [
    "To find the optimal values for the elements in $\\tilde{w}$, we need a way to measure how close our estimates made using $\\tilde{w}$ are from the correct values. We can use some pre-existing vector of labels $y$ to calibrate our model, but we need to be able to measure how far away our predictions are.\n",
    "\n",
    "Linear regression typically uses the mean squared error (MSE) as a loss function\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(\\tilde{X}\\tilde{w}-y)^{2}\n",
    "$$\n",
    "\n",
    "\n",
    "The smaller the MSE, the closer our predictions $\\hat{y}$ are to the correct values $y$ on average.\n",
    "\n",
    "We can start by choosing a random set of weights and seeing how well we do (most likely terribly). We can then determine how our weights should be adjusted to minimise the MSE function through differentiating this loss function with respect to our weights.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{2}{n}\\tilde{X}^{\\top}(\\tilde{X}\\tilde{w}-y)\n",
    "$$\n",
    "\n",
    "We can then subtract some multiple $\\eta$ of this derivative from the weights to get a new weight vector which will produce a smaller MSE. Iterating this process until the MSE stops reducing by any meaningful amount will allow us to converge on an estimate for the optimal weight vector.\n",
    "\n",
    "$$\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} - \\eta \\frac{2}{n}\\tilde{X}^{\\top}(\\tilde{X}\\tilde{w}^{(t)}-y)$$\n",
    "\n",
    "We how have all the pieces we need to begin creating a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5e9ea",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2e2fa",
   "metadata": {},
   "source": [
    "We can obtain $y$ and $X$ by subsetting our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e053e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = feature_matrix[:, -1:]\n",
    "X = feature_matrix[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa130e75",
   "metadata": {},
   "source": [
    "It is also important that our features are scaled. Without scaling, features which are expressed at higher order of magnitude (e.g. 1,000s rather than 10s) will be disproportionately weighted.\n",
    "\n",
    "We can use min-max scaling as a method to normalise our data between 0 and 1:\n",
    "\n",
    "$$\n",
    "X^{'} = \\frac{X-X_{\\text{min}}}{X_{\\text{max}}-X_{\\text{min}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "485741fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(X):\n",
    "    min_vals = X.min(axis=0)\n",
    "    max_vals = X.max(axis=0)\n",
    "    ranges = max_vals - min_vals\n",
    "    \n",
    "    ranges[ranges == 0] = 1\n",
    "    \n",
    "    return (X - min_vals) / ranges\n",
    "\n",
    "X = min_max_scaling(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f0c53",
   "metadata": {},
   "source": [
    "We can also initialise our weight vector and bias term, which we will go on to optimise later. \n",
    "\n",
    "Recall that $w \\in \\mathbb{R}^{d}$. As such, our weight vector needs to have $d$ dimensions, where $d$ is the number of predictor variables in our data set (i.e. 5). We will also need our sample size $n$ in a moment, so we can just define these values now using our design matrix (since $X \\in \\mathbb{R}^{n \\times d}$), alongside initialising $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e8c8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "np.random.seed(123) # seed for reproducibility\n",
    "\n",
    "w = np.random.rand(d, 1) # a column vector of d=5 real numbers drawn uniformly between 0 and 1.\n",
    "\n",
    "b = np.random.rand(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e20355",
   "metadata": {},
   "source": [
    "Recall that we want to obtain $\\tilde{X}$ and $\\tilde{w}$ for our vectorised calculations of $\\hat{y}$. Thus, we need to append $b$ to $w$ and append a column of $1\\text{s}$ to $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8163ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tilde_w = np.vstack([w, b])\n",
    "\n",
    "tilde_X = np.hstack([X, np.ones((n, 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e61be0",
   "metadata": {},
   "source": [
    "We can make some predictions $\\hat{y}$ on a subset of our data and compare them to the correct values $y$. We can then optimise our weights (and bias) via gradient descent and then test the performance of our constructed model on another subset of our data which the model has not yet seen.\n",
    "\n",
    "Thus, we can subset $y$ and $X$ into training and testing subsets. We want to use a fairly large proportion of our data for training, but leave a large enough proportion to test our model's performance. and $80$/$20$ split is typically used for linear regression, but this can be altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d36899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = round(1 - train_size, 1) # round(x, 1) avoids float-point errors\n",
    "\n",
    "X_train = tilde_X[:int(n * train_size)]\n",
    "X_test = tilde_X[int(n * train_size):]\n",
    "\n",
    "y_train = y[:int(n * train_size)]\n",
    "y_test = y[int(n * train_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b8444",
   "metadata": {},
   "source": [
    "With our train and test subsets created, we can begin training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a298673",
   "metadata": {},
   "source": [
    "To find the MSE of our estimates, we can create a function to calculate the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78d657f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, w, y):\n",
    "    residuals = np.matmul(X, w) - y\n",
    "    return np.mean(residuals ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe680251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3224.742047147574)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(X=X_train, w=tilde_w, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89739a55",
   "metadata": {},
   "source": [
    "We now need to perform gradient descent to optimise our weights and bias.\n",
    "\n",
    "Thus, we need to create a function which is the derivative of the MSE, with respect to the weights (+ bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e10c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_derivative(X, w, y):\n",
    "    residuals = np.matmul(X, w) - y\n",
    "    return (2 * np.matmul(X.T, residuals)) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54453896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, w, y, learning_rate, threshold, show_iterations):\n",
    "\n",
    "    MSE_change = np.inf\n",
    "    MSE_previous = np.inf\n",
    "    MSE_history = []\n",
    "    i = 0\n",
    "\n",
    "    while MSE_change > threshold:\n",
    "\n",
    "        MSE = mean_squared_error(X=X, w=w, y=y)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            MSE_history.append((i, MSE))\n",
    "\n",
    "        if i % 10_000 == 0 and show_iterations:\n",
    "            print(f\"{i:,} iterations \\n MSE = {MSE}\")\n",
    "\n",
    "        MSE_derivative = mse_derivative(X=X, w=w, y=y)\n",
    "\n",
    "        w = w - (learning_rate * MSE_derivative)\n",
    "\n",
    "        MSE_change = abs(MSE_previous - MSE)\n",
    "        MSE_previous = MSE\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return {\n",
    "        \"weights\": w, \n",
    "        \"iterations\": i, \n",
    "        \"mse\": MSE,\n",
    "        \"mse_history\": MSE_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffe59050",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = gradient_descent(X=X_train, w=tilde_w, y=y_train, learning_rate=0.0001, threshold=1e-6, show_iterations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8cb937b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, w):\n",
    "\n",
    "    y_hat = np.matmul(X, w)\n",
    "\n",
    "    MSE = mean_squared_error(X=X, w=w, y=y)\n",
    "\n",
    "    return {\n",
    "        \"y_hat\": y_hat, \n",
    "        \"mse\": MSE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "026cbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_test, y_test, gd[\"weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "edd32cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"lm_model.npy\", gd[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173943c",
   "metadata": {},
   "source": [
    "## Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7fceb",
   "metadata": {},
   "source": [
    "Below, we can see that the Numpy-only method converges to an almost identical weight vector as with scikit-learn. The main difference is that scikit-learn does not combine the bias terms into the weight vector as we did, hence the additional $0.0$ appended to the scikit-learn coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2876989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR from Scratch: [22.545121571676734, 59.58411303753044, 0.5292576989405147, 2.200442850320791, 1.475131183247628, 12.1431927656831]\n",
      "Scikit-Learn: ([22.849962262659247, 60.110817243338865, 0.6113781845223016, 2.4454624841957004, 1.754366681639631, 0.0], 11.388174475736953)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "LR from Scratch: {[float(weight[0]) for weight in gd[\"weights\"]]}\n",
    "Scikit-Learn: {[float(weight) for weight in lm.coef_[0]], float(lm.intercept_[0])}\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
